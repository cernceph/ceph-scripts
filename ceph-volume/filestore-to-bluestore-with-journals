#!/usr/bin/env bash

if [[ ! -z $1 && $1 =~ ^[0-9]+$ ]]; then
  echo "Using $1 jobs"
  if [[ ! -z $2 ]]; then
    echo "Using $2 as osd list file"
  else
    echo "Osd list file will be generated"
  fi
else
  echo "Using $1 as osd list file"
fi


echo "This host's filestore osds created by ceph-disk will be erased from the " \
     "cluster. New ones will be made with the bluestore format using ceph-volume"
read -p "Are you sure you want to proceed? (y/n): " -r

case $REPLY in
  y ) echo "Starting osd removal operation.";;
  * ) echo "Exiting..."; exit 0;;
esac

should_exit=false
for package in ceph jq parallel;
do
  if ! rpm -q $package > /dev/null; then
    echo $package " is not installed"
    should_exit=true
  fi
done
if [[ $should_exit == true ]]; then exit 1; fi

set -x

yes 'will cite' | parallel --bibtex > /dev/null 2>&1

jobs=4
if [[ ! -z $1 && $1 =~ ^[0-9]+$ ]];
then
  jobs=$1
  shift
fi

if [[ ! -z $1 ]];
then
  osd_file=$1
else
  osd_file=`mktemp --suffix=_osd_list`

  ceph-disk list --format json \
  | jq -r '.[].partitions[]? | select(.journal_dev!=null and .fs_type!="LVM2_member") | [.whoami,.path,.journal_dev] | join("\t")' \
  | sed -r "s/([a-z])[0-9]+/\1/g" > $osd_file
fi

ssds=(`cat $osd_file | awk '{ print $3 }' | sort | uniq`)
ssd_num=${#ssds[*]}

if [[ $ssd_num == 0 ]];
then
  echo "This host does not have any ssds available"
  exit 1
fi

out_and_destroy()
{
  osd_id=$1
  osd_block=$2

  if [[ -z $osd_id  ]]; then
    return 1
  fi

  if [[ `ceph osd metadata $osd_id | jq -r .osd_objectstore` == filestore ]];
  then
    ceph osd out $osd_id
    while ! ceph osd safe-to-destroy $osd_id; do sleep 10 ; done
    systemctl stop ceph-osd@$osd_id

    sleep 2

    if mount | grep -qP "/var/lib/ceph/osd/ceph-$osd_id\b"
    then
      umount -l /var/lib/ceph/osd/ceph-$osd_id || exit 1
    fi

    ceph osd destroy $osd_id --yes-i-really-mean-it || exit 1
    ceph-disk zap $osd_block || exit 1
  fi
}
export -f out_and_destroy

for ssd in ${ssds[*]};
do
  grep $ssd $osd_file | parallel -j $jobs -C '\t' out_and_destroy

  next=true
  cut -f1 < $osd_file | while read a; do
    if ! ( ceph osd tree | grep -P "osd\.$a\b" | grep destroyed );
    then
      echo "$a is not destroyed with $ssd"
      next=false
    fi
  done
  if [[ $next == false ]]; then continue; fi
  ceph-volume lvm zap $ssd
  ceph-volume lvm batch --yes $ssd `grep $ssd $osd_file | awk '{print $2}'` --osd-ids `grep $ssd $osd_file | awk '{print $1}'`
done
